# Infrastructure Components

Folder n√†y ch·ª©a c√°c component c∆° s·ªü h·∫° t·∫ßng cho h·ªá th·ªëng RAG.

## üìÅ Components

### `embeddings.py`
- **M·ª•c ƒë√≠ch**: Qu·∫£n l√Ω c√°c embedding models
- **Models h·ªó tr·ª£**: HuggingFace, OpenAI, Google
- **Usage**: `embeddings.get_embeddings(ModelType.HUGGINGFACE)`

### `llms.py`
- **M·ª•c ƒë√≠ch**: Qu·∫£n l√Ω c√°c language models
- **Models h·ªó tr·ª£**: Gemini, OpenAI, Claude
- **Usage**: `LLms.getLLm(ModelType.GEMINI)`

### `store.py`
- **M·ª•c ƒë√≠ch**: Qu·∫£n l√Ω vector store (Pinecone)
- **Ch·ª©c nƒÉng**: Upload, retrieve, search vectors
- **Usage**: `store.getRetriever(embeddings_model)`

## üîß Configuration

T·∫•t c·∫£ components ƒë∆∞·ª£c c·∫•u h√¨nh qua `config/settings.py`:

```python
from config.settings import settings

# Embedding config
embedding_model = settings.embedding.model_name

# LLM config
llm_api_key = settings.llm.gemini_api_key

# Vector store config
pinecone_api_key = settings.vector_store.pinecone_api_key
```

## üöÄ Usage Examples

```python
from infrastructure.embeddings import embeddings
from infrastructure.llms import LLms
from infrastructure.store import store
from shared.enum import ModelType

# Initialize components
embedding_model = embeddings.get_embeddings(ModelType.HUGGINGFACE)
llm = LLms.getLLm(ModelType.GEMINI)
retriever = store.getRetriever(embedding_model)

# Use in RAG pipeline
docs = retriever.get_relevant_documents("query")
response = llm.invoke("prompt")
``` 