# Data Pipeline - RAG Admissions Consulting

Folder nÃ y chá»©a toÃ n bá»™ pipeline xá»­ lÃ½ dá»¯ liá»‡u tá»« crawling Ä‘áº¿n processing cho há»‡ thá»‘ng RAG.

## ğŸ“ Cáº¥u trÃºc

```
data_pipeline/
â”œâ”€â”€ crawlers/           # Web crawlers
â”‚   â”œâ”€â”€ scraper.py     # Main scraper
â”‚   â””â”€â”€ scrape_example.py  # Example crawler
â”œâ”€â”€ processors/         # Data processors
â”‚   â””â”€â”€ data_processing.py  # Main data processor
â”œâ”€â”€ raw_data/          # Dá»¯ liá»‡u thÃ´ tá»« crawling
â”‚   â”œâ”€â”€ donga_admissions.csv
â”‚   â”œâ”€â”€ donga_admissions.json
â”‚   â””â”€â”€ donga_json_to_csv.py
â””â”€â”€ processed_data/    # Dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½
```

## ğŸ”„ Data Flow

```
Website â†’ Crawlers â†’ Raw Data â†’ Processors â†’ Processed Data â†’ Vector Store
```

## ğŸ•·ï¸ Crawlers

### `scraper.py`
- Main scraper cho website Äáº¡i há»c ÄÃ´ng Ã
- Crawl thÃ´ng tin tuyá»ƒn sinh, ngÃ nh há»c, há»c phÃ­

### `scrape_example.py`
- Example crawler Ä‘á»ƒ tham kháº£o
- Template cho viá»‡c táº¡o crawler má»›i

## âš™ï¸ Processors

### `data_processing.py`
- Xá»­ lÃ½ vÃ  lÃ m sáº¡ch dá»¯ liá»‡u thÃ´
- Chuáº©n hÃ³a format
- Táº¡o embeddings
- LÆ°u vÃ o vector store

## ğŸ“Š Data Storage

### Raw Data (`raw_data/`)
- **CSV files**: Dá»¯ liá»‡u dáº¡ng báº£ng
- **JSON files**: Dá»¯ liá»‡u cÃ³ cáº¥u trÃºc phá»©c táº¡p
- **Text files**: Ná»™i dung vÄƒn báº£n thÃ´

### Processed Data (`processed_data/`)
- **Cleaned data**: Dá»¯ liá»‡u Ä‘Ã£ lÃ m sáº¡ch
- **Embeddings**: Vector representations
- **Metadata**: ThÃ´ng tin bá»• sung

## ğŸš€ Usage

### 1. Crawling Data
```bash
# Cháº¡y main scraper
python data_pipeline/crawlers/scraper.py

# Cháº¡y example scraper
python data_pipeline/crawlers/scrape_example.py
```

### 2. Processing Data
```bash
# Xá»­ lÃ½ dá»¯ liá»‡u thÃ´
python data_pipeline/processors/data_processing.py
```

### 3. Pipeline hoÃ n chá»‰nh
```bash
# Cháº¡y toÃ n bá»™ pipeline
python run_data_pipeline.py
```

## ğŸ”§ Configuration

Cáº¥u hÃ¬nh crawling vÃ  processing trong `config/settings.py`:

```python
# Crawler settings
CRAWLER_DELAY = 1  # Delay giá»¯a cÃ¡c request (seconds)
MAX_PAGES = 100    # Sá»‘ trang tá»‘i Ä‘a Ä‘á»ƒ crawl
USER_AGENT = "RAG-Bot/1.0"

# Processor settings
CHUNK_SIZE = 1000  # KÃ­ch thÆ°á»›c chunk cho text splitting
OVERLAP = 200      # Overlap giá»¯a cÃ¡c chunk
```

## ğŸ“ Data Sources

### Äáº¡i há»c ÄÃ´ng Ã Website
- **Tuyá»ƒn sinh**: https://donga.edu.vn/tuyen-sinh
- **NgÃ nh há»c**: https://donga.edu.vn/nganh-hoc
- **Há»c phÃ­**: https://donga.edu.vn/hoc-phi
- **Tin tá»©c**: https://donga.edu.vn/tin-tuc

## ğŸ›¡ï¸ Best Practices

### Crawling Ethics
- Respect robots.txt
- Implement delays between requests
- Don't overload the server
- Cache responses when possible

### Data Quality
- Validate data after crawling
- Remove duplicates
- Handle encoding issues
- Clean HTML tags and special characters

### Error Handling
- Retry failed requests
- Log errors for debugging
- Graceful degradation
- Resume interrupted crawls

## ğŸ“ˆ Monitoring

### Crawling Metrics
- Pages crawled per hour
- Success/failure rates
- Response times
- Data quality scores

### Processing Metrics
- Processing time per document
- Memory usage
- Vector store update status
- Error rates

## ğŸ”„ Automation

### Scheduled Crawling
```bash
# Crontab example - crawl daily at 2 AM
0 2 * * * /path/to/python data_pipeline/crawlers/scraper.py
```

### CI/CD Integration
- Automated testing of crawlers
- Data validation pipelines
- Deployment to production
- Monitoring and alerting 