# RAG Admissions Consulting - Data Pipeline Flow

## ğŸ”„ Complete Data Flow Architecture

### 1. Frontend â†’ Backend â†’ Python â†’ Pinecone

```
Frontend Upload â†’ Backend API â†’ Python Scripts â†’ Vector Store
      â†“              â†“              â†“              â†“
   User Action   Process Data   AI Pipeline   Knowledge Base
```

## ğŸ“Š Data Source Types & Processing Flow

### ğŸŒ Website Crawling
```
Frontend: type="website", url="https://example.com"
   â†“
Backend: /api/v1/data-sources/upload
   â†“
Python: scraper.py â†’ JSON â†’ CSV â†’ seed.py â†’ Pinecone
   â†“
Result: Website content vectorized and searchable
```

### ğŸ“„ PDF File Upload
```
Frontend: type="pdf", file=document.pdf
   â†“
Backend: Save file â†’ /uploads/timestamp-document.pdf
   â†“
Python: data_processing.py â†’ Extract text â†’ CSV â†’ seed.py â†’ Pinecone
   â†“
Result: PDF content vectorized and searchable
```

### ğŸ“Š CSV File Upload
```
Frontend: type="csv", file=data.csv
   â†“
Backend: Save file â†’ /uploads/timestamp-data.csv
   â†“
Python: data_processing.py â†’ Clean format â†’ CSV â†’ seed.py â†’ Pinecone
   â†“
Result: CSV content vectorized and searchable
```

### âœï¸ Manual Q&A Input
```
Frontend: type="manual", title="Question", content="Answer"
   â†“
Backend: Store in database
   â†“
Python: manual_processor.py â†’ Format â†’ CSV â†’ seed.py â†’ Pinecone
   â†“
Result: Manual content vectorized and searchable
```

## ğŸ”§ Technical Implementation

### Backend API Endpoint
```typescript
POST /api/v1/data-sources/upload
Content-Type: multipart/form-data

Body:
- type: "website" | "pdf" | "csv" | "manual"
- name: string
- description?: string
- file?: File (for pdf/csv)
- url?: string (for website)
- title?: string (for manual)
- content?: string (for manual)
```

### Python Scripts Called

#### 1. Unified Pipeline Runner
```bash
python scripts/run_data_pipeline.py <data_source_id> <type> <input>
```

#### 2. Individual Scripts (called by pipeline)
```bash
# Website crawling
python data_pipeline/crawlers/scraper.py "url" "data_source_id"

# File processing  
python data_pipeline/processors/data_processing.py "file_path" "data_source_id"

# Manual input
python data_pipeline/processors/manual_processor.py '{"title":"Q","content":"A"}' "data_source_id"

# Vector store upload
python scripts/seed.py "csv_file_path" "data_source_id"
```

## ğŸ“ File Structure & Outputs

### Generated Files
```
data_pipeline/
â”œâ”€â”€ raw_data/
â”‚   â””â”€â”€ scraped_{data_source_id}.json     # Raw scraped data
â”œâ”€â”€ processed_data/
â”‚   â”œâ”€â”€ scraped_{data_source_id}.csv      # Website â†’ CSV
â”‚   â”œâ”€â”€ pdf_{data_source_id}.csv          # PDF â†’ CSV
â”‚   â”œâ”€â”€ csv_{data_source_id}.csv          # CSV â†’ Standardized CSV
â”‚   â””â”€â”€ manual_{data_source_id}.csv       # Manual â†’ CSV
â””â”€â”€ logs/
    â””â”€â”€ data_pipeline_YYYY-MM-DD.log     # Processing logs
```

### CSV Format (Standard Output)
```csv
Text
"University admission requirements include..."
"Tuition fees for 2024 academic year..."
"Application deadline is March 15th..."
```

## ğŸ—„ï¸ Database Schema

### DataSource Table
```sql
CREATE TABLE data_source (
    id UUID PRIMARY KEY,
    name VARCHAR(500) NOT NULL,
    description TEXT,
    type ENUM('web_crawl', 'file_upload', 'manual_input') NOT NULL,
    source_url VARCHAR(2000),
    file_path VARCHAR(1000),
    status ENUM('pending', 'processing', 'completed', 'failed') DEFAULT 'pending',
    documents_count INTEGER DEFAULT 0,
    vectors_count INTEGER DEFAULT 0,
    uploaded_by VARCHAR(100) NOT NULL,
    uploader_email VARCHAR(255) NOT NULL,
    processing_started_at TIMESTAMPTZ,
    processing_completed_at TIMESTAMPTZ,
    error_message TEXT,
    metadata JSONB,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);
```

## ğŸš€ Processing Status Flow

### Status Transitions
```
pending â†’ processing â†’ completed âœ…
        â†˜            â†˜ failed âŒ
```

### Status Updates
1. **pending**: Data source created, waiting for processing
2. **processing**: Python scripts are running
3. **completed**: Successfully processed and uploaded to Pinecone
4. **failed**: Error occurred during processing

## ğŸ“ˆ Metrics Tracking

### What Gets Counted
- **documents_count**: Number of text chunks processed
- **vectors_count**: Number of embeddings created in Pinecone
- **processing_time**: Duration from start to completion
- **file_size**: Original file size (for uploads)

### Example Metrics
```json
{
  "documents_count": 45,
  "vectors_count": 180,
  "processing_time": "00:02:30",
  "metadata": {
    "originalName": "handbook.pdf",
    "fileSize": 2048576,
    "pages_processed": 15
  }
}
```

## ğŸ” Error Handling

### Common Error Scenarios
1. **Invalid file format**: Only PDF/CSV allowed
2. **Empty content**: No extractable text found
3. **Network timeout**: Website unreachable
4. **Pinecone quota**: Vector store limits exceeded
5. **Processing timeout**: Script takes too long

### Error Response Format
```json
{
  "status": "failed",
  "error_message": "No text extracted from PDF file",
  "processing_completed_at": "2024-01-01T12:00:00Z"
}
```

## ğŸ¯ Frontend Integration Examples

### JavaScript Upload Functions
```javascript
// Universal upload function
async function uploadDataSource(type, formData) {
  const response = await fetch('/api/v1/data-sources/upload', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${token}`
    },
    body: formData
  });
  
  return response.json();
}

// Website crawling
const websiteData = new FormData();
websiteData.append('type', 'website');
websiteData.append('name', 'University Website');
websiteData.append('url', 'https://university.edu');

// File upload
const fileData = new FormData();
fileData.append('type', 'pdf');
fileData.append('name', 'Admission Guide');
fileData.append('file', fileInput.files[0]);

// Manual input
const manualData = new FormData();
manualData.append('type', 'manual');
manualData.append('name', 'FAQ Entry');
manualData.append('title', 'What are the admission requirements?');
manualData.append('content', 'Students need minimum GPA 3.0...');
```

## âš¡ Performance Considerations

### Processing Times (Estimated)
- **Manual input**: ~5 seconds
- **Small CSV (< 100 rows)**: ~30 seconds  
- **PDF (< 50 pages)**: ~1-2 minutes
- **Website crawl (< 100 pages)**: ~3-5 minutes

### Optimization Tips
1. **Async processing**: Don't block frontend
2. **Progress tracking**: Show real-time status
3. **Batch uploads**: Group similar content
4. **Cache results**: Avoid reprocessing same content
5. **Error recovery**: Retry failed uploads

## ğŸ”’ Security & Validation

### Input Validation
- File size limits (< 50MB)
- File type restrictions (.pdf, .csv only)
- URL validation (https only)
- Content length limits
- XSS protection for manual input

### Access Control
- JWT authentication required
- User-specific data sources
- Admin-only bulk operations
- Rate limiting on uploads

This complete flow ensures reliable, scalable, and user-friendly data ingestion for the RAG system! ğŸš€ 